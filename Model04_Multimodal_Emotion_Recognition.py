# -*- coding: utf-8 -*-
"""AhmedAbdelsamad_202201913_M3_XAI_(1)_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10FceMr079I2kPnB9tOT7gv2SV0JegH2T

# **Real Time Multimodal Human Emotion Recognition: A Deep Learning Approach**
"""

!pip install tf-keras-vis lime mtcnn transformers

import os
from pathlib import Path
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, ConvLSTM2D, Attention, TimeDistributed, Concatenate, LSTM
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.inspection import permutation_importance
from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus
from tf_keras_vis.utils.model_modifiers import ReplaceToLinear
from tf_keras_vis.utils.scores import CategoricalScore
import shap
from lime import lime_image
from skimage.segmentation import mark_boundaries
from skimage.feature import hog
from skimage import exposure
import kagglehub
import gc
from mtcnn import MTCNN
#from feat.detector import Detector
#from gaze_tracking import GazeTracking
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torch
from tf_keras_vis.utils.scores import BinaryScore

!pip install librosa

import librosa
import librosa.display

def Map_To_Threat_Level(Emotion):
  High_Threat_Emotions = ['angry' , 'disgust' , 'fear']
  Low_Threat_Emotions = ['happy' , 'neutral' , 'sad' , 'surprise']
  if Emotion in High_Threat_Emotions:
    return 'High Threat'
  else:
    return 'Low Threat'

def Preprocess_Face_Expression_Dataset(Imgs_Path , Img_Size=(128,128) , Sample_frac = 0.05):

  Emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
  Data = {
      'Image_Paths': [],
      'Emotions': [],
      'Split': [],
      'Is_Augmented': [],
      'Threat_Level': [],
      'Image_Name': []
  }

  Data_Agm = ImageDataGenerator(
      rotation_range=20,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest'
  )

  All_Images = []

  Output_Dir = "Processed_Images"
  os.makedirs(Output_Dir, exist_ok=True)

  for Split in ['train', 'validation']:
      Split_Path = os.path.join(Imgs_Path, 'images', Split)
      if not os.path.exists(Split_Path):
          print(f"Directory '{Split_Path}' does not exist.")
          continue
      for Emotion in Emotions:
          Emotion_Path = os.path.join(Split_Path, Emotion)
          if not os.path.exists(Emotion_Path):
              print(f"Directory '{Emotion_Path}' does not exist.")
              continue
          for Image_Name in os.listdir(Emotion_Path):
              if not Image_Name.endswith(('.jpg', '.jpeg', '.png')):
                  continue
              Image_Path = os.path.join(Emotion_Path, Image_Name)
              All_Images.append({
                  'Image_Path': Image_Path,
                  'Emotion': Emotion,
                  'Split': Split,
                  'Image_Name': Image_Name
              })

  df_All = pd.DataFrame(All_Images)
  df_Sample = df_All.groupby(['Emotion', 'Split'], group_keys=False).apply(
      lambda x: x.sample(frac=Sample_frac, random_state=42)
  ).reset_index(drop=True)

  print(f"Sampled {len(df_Sample)} Images from {len(df_All)} Total Images.")

  df_Sample['Threat_Level'] = df_Sample['Emotion'].apply(Map_To_Threat_Level)

  for _, Row in df_Sample.iterrows():
      Image_Path = Row['Image_Path']
      Emotion = Row['Emotion']
      Split = Row['Split']
      Image_Name = Row['Image_Name']
      Threat_Level = Row['Threat_Level']

      Image = cv2.imread(Image_Path, cv2.IMREAD_GRAYSCALE)
      if Image is None:
          continue

      Image_Resized = cv2.resize(Image, Img_Size)

      output_path = os.path.join(Output_Dir, f"{Split}_{Emotion}_{Image_Name}")
      cv2.imwrite(output_path, Image_Resized)
      Data['Image_Paths'].append(output_path)
      Data['Emotions'].append(Emotion)
      Data['Split'].append(Split)
      Data['Is_Augmented'].append(False)
      Data['Threat_Level'].append(Threat_Level)
      Data['Image_Name'].append(Image_Name)

      Image_Reshaped = Image_Resized.reshape(1, Img_Size[0], Img_Size[1], 1)

      Augmented_Images = Data_Agm.flow(Image_Reshaped, batch_size=1)

      for i in range(3):

          Augmented_Image = next(Augmented_Images)[0].astype('uint8')
          Aug_Path = os.path.join(Output_Dir, f"{Split}_{Emotion}_Aug{i}_{Image_Name}")
          cv2.imwrite(Aug_Path , Augmented_Image)
          Data['Image_Paths'].append(Aug_Path)
          Data['Emotions'].append(Emotion)
          Data['Split'].append(Split)
          Data['Is_Augmented'].append(True)
          Data['Threat_Level'].append(Threat_Level)
          Data['Image_Name'].append(Image_Name)

  df = pd.DataFrame(Data)
  return df

def Face_Expression_EDA(Dataset_Name):

    df = pd.read_csv(Dataset_Name)
    print(f"Total Images in {Dataset_Name} : {len(df)}")
    print(f"Total Images in Train : {len(df[df['Split'] == 'train'])}")
    print(f"Total Images in Validation : {len(df[df['Split'] == 'validation'])}")
    print()

    plt.figure(figsize=(10, 6))
    sns.countplot(data=df, x='Emotions', hue='Split')
    plt.title('Distribution of Emotions Across Splits')
    plt.xlabel('Emotions')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()
    print()

    plt.figure(figsize=(10, 6))
    sns.countplot(data=df, x='Threat_Level', hue='Split')
    plt.title('Distribution of Threat Levels Across Splits')
    plt.xlabel('Threat Level')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()
    print()

    Emotions = df['Emotions'].unique()
    print(f"Emotions in Dataset : {Emotions}")
    print()


    Num_Emotions = len(Emotions)
    Num_Cols = 4
    Num_Rows = (Num_Emotions + Num_Cols - 1) // Num_Cols


    for Idx, Emotion in enumerate(Emotions):

        Emotion_Images = df[df['Emotions'] == Emotion]['Image_Paths'].unique()

        if len(Emotion_Images) > 0:
            Img_Path = Emotion_Images[0]
            plt.figure()
            Img = cv2.imread(Img_Path, cv2.IMREAD_GRAYSCALE)
            plt.imshow(Img, cmap=plt.cm.gray)
            plt.title(Emotion)
            plt.axis('off')
            plt.show()

def Hog_Features(Dataset , Num_Samples = 100 , Num_Visualize = 10):

  df = pd.read_csv(Dataset)

  Emotions = df['Emotions'].unique()
  print(f"Emotions in Dataset : {Emotions}")
  print()

  Sample_per_Emotion = max(1 , Num_Samples // len(Emotions))
  print(f"Sample per Emotion : {Sample_per_Emotion}")

  df_Sample = df.groupby('Emotions' , group_keys=False).apply(
      lambda x: x.sample(n=min(Sample_per_Emotion , len(x))
      , random_state=42)).reset_index(drop=True)

  print(f"Total Samples in Dataset : {len(df_Sample)}")
  print()

  if len(df_Sample) < Num_Samples:
    Remaining = Num_Samples - len(df_Sample)
    Additional = df.drop(df_Sample.index).sample(n=Remaining , random_state=42)
    df_Sample = pd.concat([df_Sample , Additional] , ignore_index=True)


  df_Sample = df_Sample.sample(frac=1 , random_state=42).reset_index(drop=True)
  print(f"Total Samples in Sampled Dataset : {len(df_Sample)}")
  print()

  Hog_Features_Data = []
  Hog_Images = []

  for Img_Path in df_Sample['Image_Paths']:
    Img = cv2.imread(Img_Path, cv2.IMREAD_GRAYSCALE)
    Hog_Features, Hog_Img = hog(
        Img,
        orientations=9,
        pixels_per_cell=(8, 8),
        cells_per_block=(2, 2),
        visualize=True,
        block_norm='L2-Hys'
    )

    Hog_Features_Data.append(Hog_Features)
    Hog_Images.append(Hog_Img)


  Visualize_Indices = df_Sample.sample(n=Num_Visualize , random_state=42).index
  print(f"Visualize Indices : {Visualize_Indices}")
  print()

  fig , axes = plt.subplots(nrows= 5 , ncols= 4 , figsize=(15 , 20) ,
                           sharex=False , sharey=False)
  axes = axes.flatten()


  for Idx , Sample_Idx in enumerate(Visualize_Indices):
    Img_Path = df_Sample.loc[Sample_Idx , 'Image_Paths']
    Emotion = df_Sample.loc[Sample_Idx , 'Emotions']
    Img = cv2.imread(Img_Path , cv2.IMREAD_GRAYSCALE)

    ax1 = axes[Idx * 2]
    ax1.axis('off')
    ax1.imshow(Img , cmap=plt.cm.gray)
    ax1.set_title(f'Original Image : {Emotion}')

    ax2 = axes[Idx * 2 + 1]
    Hog_Image_Rescaled = exposure.rescale_intensity(Hog_Images[Sample_Idx] , in_range=(0 , 10))
    ax2.axis('off')
    ax2.imshow(Hog_Image_Rescaled , cmap=plt.cm.gray)
    ax2.set_title('Hog Visualization')


  plt.show()

  df_Sample['Hog_Features'] = Hog_Features_Data
  df_Sample.to_csv('Images_Dataset_Hog.csv' , index=False)

  return df_Sample

def Preprocess_RAVDESS_Dataset(DATA_DIR, Output_DIR, Sample_Frac=1.0, Sr=16000, N_MFCC=13, Max_Length=200, Normalize=True):

    Feature_DIR = os.path.join(Output_DIR, 'Features')
    os.makedirs(Feature_DIR, exist_ok=True)

    print("Preprocessing RAVDESS Audio Dataset:")

    processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
    wav2vec_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

    Data = {
        'Audio_Paths': [],
        'Feature_Paths': [],
        'Wav2Vec_Paths': [],
        'Labels': [],
        'Actor': [],
        'Gender': [],
        'Audio_Name': []
    }

    Labels = {
        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'
    }

    Audio_Files = []
    for root, _, files in os.walk(DATA_DIR):
        Audio_Files.extend(os.path.join(root, f) for f in files if f.endswith('.wav'))

    print(f"Found {len(Audio_Files)} audio files in {DATA_DIR}")
    print("\n")

    if not Audio_Files:
        print("No audio files found in the dataset.")
        return pd.DataFrame(Data)

    File_Records = []

    for Audio_Path in Audio_Files:
        Audio_Name = os.path.splitext(os.path.basename(Audio_Path))[0]
        Emotion_Code = Audio_Name.split('-')[2]
        Label = Labels.get(Emotion_Code, 'Unknown')
        Actor = Audio_Name.split('-')[6]
        Gender = 'Male' if int(Actor) % 2 == 1 else 'Female'
        File_Records.append({
            'Audio_Path': Audio_Path,
            'Audio_Name': Audio_Name,
            'Emotion_Code': Emotion_Code,
            'Label': Label,
            'Actor': Actor,
            'Gender': Gender
        })

    df_All = pd.DataFrame(File_Records)
    Actors = df_All['Actor'].unique()
    print(f"Found {len(Actors)} actors in the dataset.")
    print("\n")

    Total_Samples = max(len(Actors), int(len(Audio_Files) * Sample_Frac))
    Sampled_Actors = Actors
    print(f"Processing files from {len(Sampled_Actors)} actors.")
    print("\n")

    Sampled_Files = []

    for Actor in Sampled_Actors:
        Actor_Files = df_All[df_All['Actor'] == Actor]
        Num_Files = len(Actor_Files)
        Num_Samples = max(1, int(Num_Files * Sample_Frac))
        Sampled = Actor_Files.sample(n=Num_Samples, random_state=42)
        Sampled_Files.append(Sampled)

    df_sampled = pd.concat(Sampled_Files).reset_index(drop=True)
    print(f"Sampled {len(df_sampled)} audio files across {len(Sampled_Actors)} actors.")
    print("\n")

    for _, Row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc="Processing Audio Files"):
        Audio_Path = Row['Audio_Path']
        Audio_Name = Row['Audio_Name']
        Label = Row['Label']
        Actor = Row['Actor']
        Gender = Row['Gender']

        if Label == 'Unknown':
            print(f"Could not infer label for {Audio_Path}, skipping.")
            continue

        print(f"Processing audio: {Audio_Path}, Label: {Label}")

        Y, _ = librosa.load(Audio_Path, sr=Sr)
        Y, _ = librosa.effects.trim(Y)

        if Normalize:
            Y = Y / np.max(np.abs(Y))

        MFCCS = librosa.feature.mfcc(y=Y, sr=Sr, n_mfcc=N_MFCC)

        if MFCCS.shape[1] < Max_Length:
            MFCCS = np.pad(MFCCS, ((0, 0), (0, Max_Length - MFCCS.shape[1])), mode='constant')
        else:
            MFCCS = MFCCS[:, :Max_Length]

        if MFCCS is None:
            print(f"Failed to extract MFCCs from {Audio_Path}, skipping.")
            continue

        inputs = processor(Y, sampling_rate=Sr, return_tensors="pt", padding=True)
        with torch.no_grad():
            Wav2Vec_Features = wav2vec_model(inputs.input_values).last_hidden_state.squeeze().numpy()

        Feature_Path = os.path.join(Feature_DIR, f"{Audio_Name}_mfcc.npy")
        Wav2Vec_Path = os.path.join(Feature_DIR, f"{Audio_Name}_wav2vec.npy")
        np.save(Feature_Path, MFCCS)
        np.save(Wav2Vec_Path, Wav2Vec_Features)
        print(f"Saved features: {Feature_Path}")
        print(f"Saved Wav2Vec features: {Wav2Vec_Path}")

        Data['Audio_Paths'].append(Audio_Path)
        Data['Feature_Paths'].append(Feature_Path)
        Data['Wav2Vec_Paths'].append(Wav2Vec_Path)
        Data['Labels'].append(Label)
        Data['Actor'].append(Actor)
        Data['Gender'].append(Gender)
        Data['Audio_Name'].append(Audio_Name)

    df = pd.DataFrame(Data)
    print(f"Processed {len(df)} audio files.")
    print(f"Label distribution:\n{df['Labels'].value_counts()}")
    print(f"Gender distribution:\n{df['Gender'].value_counts()}")
    print("\n")

    Output_Data_Path = os.path.join(Output_DIR, 'RAVDESS_Dataset.csv')
    df.to_csv(Output_Data_Path, index=False)
    print(f"Metadata saved to {Output_Data_Path}")
    print("\n")

    return df

def EDA_RAVDESS_Dataset(Dataset_Path, Num_Visualizations=5):

    df = pd.read_csv(Dataset_Path)

    print("Label Distribution:")
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='Labels', hue='Labels')
    plt.title('Label Distribution')
    plt.xlabel('Labels')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()
    print("\n")

    print("\nGender Distribution:")
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='Gender', hue='Gender')
    plt.title('Gender Distribution')
    plt.xlabel('Gender')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nActor Distribution:")
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='Actor', hue='Actor')
    plt.title('Actor Distribution')
    plt.xlabel('Actor')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nAudio Length Distribution:")
    df['Audio_Length'] = df['Audio_Paths'].apply(lambda x: librosa.get_duration(filename=x))
    plt.figure(figsize=(8, 6))
    sns.histplot(data=df, x='Audio_Length', bins=30, kde=True)
    plt.title('Audio Length Distribution')
    plt.xlabel('Audio Length (seconds)')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nLabel vs Gender Cross-Tabulation:")
    cross_tab_gender = pd.crosstab(df['Labels'], df['Gender'])
    plt.figure(figsize=(8, 6))
    sns.heatmap(cross_tab_gender, annot=True, fmt='d', cmap='Blues')
    plt.title('Labels vs Gender')
    plt.xlabel('Gender')
    plt.ylabel('Labels')
    plt.show()
    print("\n")

    print("\n Visualizing Sample Audio Spectrograms : ")
    Sample_Audios = df.sample(n=Num_Visualizations, random_state=42)

    for Idx, Audio_Path in enumerate(Sample_Audios['Audio_Paths']):
      Y, _ = librosa.load(Audio_Path , sr =22050)
      S = librosa.feature.melspectrogram(y=Y, sr=22050)
      plt.figure(figsize=(8, 6))
      librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='log', x_axis='time')
      plt.title(f'Sample Audio Spectrogram {Idx + 1}')
      plt.colorbar(format='%+2.0f dB')
      plt.show()
      print("\n")

    print("\n")

    print("\nVisualizing Sample Audio Waveforms : ")
    Sample_Audios = df.sample(n=Num_Visualizations, random_state=42)
    for Idx, Audio_Path in enumerate(Sample_Audios['Audio_Paths']):
        Y, _ = librosa.load(Audio_Path , sr = 22050)
        plt.figure(figsize=(8, 6))
        librosa.display.waveshow(Y, sr=22050)
        plt.title(f'Sample Audio Waveform {Idx + 1}')
        plt.show()
        print("\n")

    print("\n")

    print("\nMFCC Visualization : ")
    Sample_Features = df.sample(n=Num_Visualizations, random_state=42)
    for Idx, Feature_Path in enumerate(Sample_Features['Feature_Paths']):
        MFCCs = np.load(Feature_Path)
        plt.figure(figsize=(8, 6))
        librosa.display.specshow(MFCCs, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'Sample MFCCs {Idx + 1}')
        plt.show()
        print("\n")

    print("\n")

def RAVDESS_Feature_Engeenring(Dataset , Output_Dir):

    df = pd.read_csv(Dataset)

    Spectral_Centroid_Data = []
    Spectral_Rolloff_Data = []
    Spectral_Flatness_Data = []
    ZCR_Data = []

    for Audio_Path in tqdm(df['Audio_Paths'] , desc="Processing Audio Files"):

        Y, _ = librosa.load(Audio_Path, sr=22050)

        # Spectral Centroid
        Centroid = librosa.feature.spectral_centroid(y=Y, sr=22050)
        Centroid_Mean = np.mean(Centroid)
        Spectral_Centroid_Data.append(Centroid_Mean)

        # Spectral Rolloff
        Rolloff = librosa.feature.spectral_rolloff(y=Y, sr=22050)
        Rolloff_Mean = np.mean(Rolloff)
        Spectral_Rolloff_Data.append(Rolloff_Mean)

        # Spectral Flatness
        Flatness = librosa.feature.spectral_flatness(y=Y)
        Flatness_Mean = np.mean(Flatness)
        Spectral_Flatness_Data.append(Flatness_Mean)

        # Zero Crossing Rate
        ZCR = librosa.feature.zero_crossing_rate(y=Y)
        ZCR_Mean = np.mean(ZCR)
        ZCR_Data.append(ZCR_Mean)

    df['Spectral_Centroid'] = Spectral_Centroid_Data
    df['Spectral_Rolloff'] = Spectral_Rolloff_Data
    df['Spectral_Flatness'] = Spectral_Flatness_Data
    df['ZCR'] = ZCR_Data

    df.to_csv(os.path.join(Output_Dir , 'RAVDESS_Dataset_Features.csv') , index=False)
    print(f"Features saved to {os.path.join(Output_Dir , 'RAVDESS_Dataset_Features.csv')}")

    return df

def Plot_Features(Dataset):
    df = pd.read_csv(Dataset)

    Features = ['Spectral_Centroid' , 'Spectral_Rolloff' , 'Spectral_Flatness' , 'ZCR']
    for Feature in Features:
        plt.figure(figsize=(8, 6))
        sns.histplot(data=df, x=Feature, bins=30, kde=True)
        plt.title(f'{Feature} Distribution')
        plt.xlabel(Feature)
        plt.ylabel('Count')
        plt.show()
        print("\n")

        print(f"\n{Feature} by Emotion:")
        plt.figure(figsize=(10, 6))
        sns.boxplot(data=df, x='Labels', y=Feature)
        plt.title(f'{Feature} by Emotion')
        plt.xlabel('Emotion')
        plt.ylabel(Feature)
        plt.xticks(rotation=45)
        plt.show()
        print("\n")

        print(f"\n{Feature} by Gender:")
        plt.figure(figsize=(8, 6))
        sns.boxplot(data=df, x='Gender', y=Feature)
        plt.title(f'{Feature} by Gender')
        plt.xlabel('Gender')
        plt.ylabel(Feature)
        plt.show()
        print("\n")


    print("\nCorrelation Heatmap of Features : ")
    plt.figure(figsize=(10, 8))
    sns.heatmap(df[Features].corr(), annot=True, cmap='coolwarm', fmt=".2f" , vmax=1 , vmin=-1)
    plt.title('Correlation Heatmap of Features')
    plt.show()
    print("\n")

def Visualize_Features(Dataset , Num_Visualizations = 2):

  df = pd.read_csv(Dataset)

  Features = ['Spectral_Centroid' , 'Spectral_Rolloff' , 'Spectral_Flatness' , 'ZCR']

  Sample_Features = df.sample(n=Num_Visualizations , random_state=42)

  for Idx , Feature in enumerate(Features):
    print(f"Feature : {Feature}")

    Audio_Path = Sample_Features['Audio_Paths'].iloc[Idx % len(Sample_Features)]
    Y , _ = librosa.load(Audio_Path , sr=22050)
    Times = librosa.times_like(Y , sr=22050)

    plt.figure(figsize=(10, 6))
    plt.plot(Times , Y , color='blue')
    plt.title(f'Time Series Plot of {Feature}')
    plt.xlabel('Time')
    plt.ylabel(Feature)
    plt.show()
    print("\n")

  print("\n")

Imgs_Path = kagglehub.dataset_download("jonathanoheix/face-expression-recognition-dataset")
print("Path to dataset files:", Imgs_Path)

print(f"Directory Contents : {os.listdir(Imgs_Path)}")

print(f"Subdirectories in Images Contents : {os.listdir(os.path.join(Imgs_Path, 'images'))}")

Images_Dataset = Preprocess_Face_Expression_Dataset(Imgs_Path)
Images_Dataset.to_csv('Images_Dataset.csv' , index=False)

Image_Dataset = 'Images_Dataset.csv'
Face_Expression_EDA(Image_Dataset)

Feature_Face_Expresion = Hog_Features('Images_Dataset.csv')

Audio_Path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")
print("Path to dataset files:", Audio_Path)

Audio_Dataset = Preprocess_RAVDESS_Dataset(Audio_Path, Output_DIR='Processed_Ravdess' , Sample_Frac=0.1)

Dataset_Audio = 'Processed_Ravdess/RAVDESS_Dataset.csv'
EDA_RAVDESS_Dataset(Dataset_Audio)

Features_Ravdess = RAVDESS_Feature_Engeenring('Processed_Ravdess/RAVDESS_Dataset.csv' , 'Processed_Ravdess')

Features_Ravdess_Dataset = 'Processed_Ravdess/RAVDESS_Dataset_Features.csv'
Plot_Features(Features_Ravdess_Dataset)

Visualize_Features(Features_Ravdess_Dataset)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Bidirectional, Attention, Concatenate, Permute, Multiply
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import os
import cv2
from tqdm import tqdm

def load_preprocessed_data(image_csv_path, audio_csv_path, img_size=(128, 128), max_length=200, max_wav2vec_length=300, samples_per_emotion=100):

    df_images = pd.read_csv(image_csv_path)
    df_images = df_images[df_images['Is_Augmented'] == False]
    df_audio = pd.read_csv(audio_csv_path)

    common_emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
    df_images = df_images[df_images['Emotions'].isin(common_emotions)]
    df_audio = df_audio[df_audio['Labels'].isin(common_emotions)]

    images = []
    mfccs = []
    wav2vec_features = []
    labels = []
    threat_levels = []

    for emotion in common_emotions:
        df_img_emotion = df_images[df_images['Emotions'] == emotion]
        df_img_sample = df_img_emotion.sample(n=min(samples_per_emotion, len(df_img_emotion)), random_state=42) if len(df_img_emotion) > 0 else pd.DataFrame()

        df_audio_emotion = df_audio[df_audio['Labels'] == emotion]
        df_audio_sample = df_audio_emotion.sample(n=min(samples_per_emotion, len(df_audio_emotion)), random_state=42) if len(df_audio_emotion) > 0 else pd.DataFrame()

        for _, row in df_img_sample.iterrows():
            img_path = row['Image_Paths']
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                continue
            img = cv2.resize(img, img_size) / 255.0
            images.append(img)
            labels.append(row['Emotions'])
            threat_levels.append(row['Threat_Level'])

            if not df_audio_sample.empty:
                audio_row = df_audio_sample.iloc[0]
                mfcc = np.load(audio_row['Feature_Paths'])
                wav2vec = np.load(audio_row['Wav2Vec_Paths'])

                if mfcc.shape[1] > max_length:
                    mfcc = mfcc[:, :max_length]
                else:
                    mfcc = np.pad(mfcc, ((0, 0), (0, max_length - mfcc.shape[1])), mode='constant')

                if wav2vec.ndim == 3:
                    wav2vec = wav2vec.squeeze(axis=0)
                seq_len = wav2vec.shape[0]
                if seq_len > max_wav2vec_length:
                    wav2vec = wav2vec[:max_wav2vec_length, :]
                else:
                    wav2vec = np.pad(wav2vec, ((0, max_wav2vec_length - seq_len), (0, 0)), mode='constant')

                mfccs.append(mfcc)
                wav2vec_features.append(wav2vec)
            else:

                images.pop()
                labels.pop()
                threat_levels.pop()

        for _, row in df_audio_sample.iterrows():
            mfcc = np.load(row['Feature_Paths'])
            wav2vec = np.load(row['Wav2Vec_Paths'])

            if mfcc.shape[1] > max_length:
                mfcc = mfcc[:, :max_length]
            else:
                mfcc = np.pad(mfcc, ((0, 0), (0, max_length - mfcc.shape[1])), mode='constant')

            if wav2vec.ndim == 3:
                wav2vec = wav2vec.squeeze(axis=0)
            seq_len = wav2vec.shape[0]
            if seq_len > max_wav2vec_length:
                wav2vec = wav2vec[:max_wav2vec_length, :]
            else:
                wav2vec = np.pad(wav2vec, ((0, max_wav2vec_length - seq_len), (0, 0)), mode='constant')

            mfccs.append(mfcc)
            wav2vec_features.append(wav2vec)
            labels.append(row['Labels'])
            threat_levels.append(Map_To_Threat_Level(row['Labels']))

            if not df_img_sample.empty:
                img_row = df_img_sample.iloc[0]
                img = cv2.imread(img_row['Image_Paths'], cv2.IMREAD_GRAYSCALE)
                if img is None:
                    mfccs.pop()
                    wav2vec_features.pop()
                    labels.pop()
                    threat_levels.pop()
                    continue
                img = cv2.resize(img, img_size) / 255.0
                images.append(img)
            else:

                mfccs.pop()
                wav2vec_features.pop()
                labels.pop()
                threat_levels.pop()

    images = np.array(images)[..., np.newaxis]
    mfccs = np.array(mfccs)
    wav2vec_features = np.array(wav2vec_features)

    le_emotion = LabelEncoder()
    le_threat = LabelEncoder()
    labels = le_emotion.fit_transform(labels)
    threat_levels = le_threat.fit_transform(threat_levels)

    lengths = [len(images), len(mfccs), len(wav2vec_features), len(labels), len(threat_levels)]
    if len(set(lengths)) > 1:
        raise ValueError(f"Inconsistent sample counts: {lengths}")

    print(f"Loaded {len(images)} samples.")
    return images, mfccs, wav2vec_features, labels, threat_levels, le_emotion, le_threat

def build_multimodal_model(img_size=(128, 128), n_mfcc=13, max_length=200, max_wav2vec_length=300, wav2vec_dim=768, num_emotions=7, num_threat_levels=2):

    # Facial branch (CNN)
    img_input = Input(shape=(img_size[0], img_size[1], 1), name='image_input')
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    img_features = Flatten()(x)
    img_features = Dense(256, activation='relu')(img_features)

    # Audio MFCC branch (BiLSTM)
    mfcc_input = Input(shape=(n_mfcc, max_length), name='mfcc_input')
    mfcc_reshaped = Permute((2, 1))(mfcc_input)
    mfcc_lstm = Bidirectional(LSTM(128, return_sequences=True))(mfcc_reshaped)
    mfcc_attention = Attention()([mfcc_lstm, mfcc_lstm])
    mfcc_features = Flatten()(mfcc_attention)
    mfcc_features = Dense(256, activation='relu')(mfcc_features)

    # Wav2Vec branch (BiLSTM)
    wav2vec_input = Input(shape=(max_wav2vec_length, wav2vec_dim), name='wav2vec_input')
    wav2vec_lstm = Bidirectional(LSTM(128, return_sequences=False))(wav2vec_input)  # Output: (batch, 256)
    wav2vec_features = Dense(256, activation='relu')(wav2vec_lstm)

    combined_features = Concatenate()([img_features, mfcc_features, wav2vec_features])
    combined_features = Dense(512, activation='relu')(combined_features)
    fusion_attention = Dense(512, activation='softmax')(combined_features)
    fused_features = Multiply()([combined_features, fusion_attention])

    emotion_output = Dense(num_emotions, activation='softmax', name='emotion_output')(fused_features)
    threat_output = Dense(num_threat_levels, activation='softmax', name='threat_output')(fused_features)

    model = Model(inputs=[img_input, mfcc_input, wav2vec_input], outputs=[emotion_output, threat_output])
    model.compile(
        optimizer=Adam(learning_rate=1e-3),
        loss={'emotion_output': 'sparse_categorical_crossentropy', 'threat_output': 'sparse_categorical_crossentropy'},
        metrics={'emotion_output': 'accuracy', 'threat_output': 'accuracy'}
    )

    return model

def train_multimodal_model(model, images, mfccs, wav2vec_features, labels, threat_levels, batch_size=32, epochs=20, validation_split=0.2):

    history = model.fit(
        x={'image_input': images, 'mfcc_input': mfccs, 'wav2vec_input': wav2vec_features},
        y={'emotion_output': labels, 'threat_output': threat_levels},
        batch_size=batch_size,
        epochs=epochs,
        validation_split=validation_split,
        verbose=1
    )
    return history

def evaluate_multimodal_model(model, images, mfccs, wav2vec_features, labels, threat_levels):

    metrics = model.evaluate(
        x={'image_input': images, 'mfcc_input': mfccs, 'wav2vec_input': wav2vec_features},
        y={'emotion_output': labels, 'threat_output': threat_levels},
        verbose=0
    )
    return {
        'emotion_loss': metrics[0],
        'threat_loss': metrics[1],
        'emotion_accuracy': metrics[2],
        'threat_accuracy': metrics[3]
    }

image_csv_path = 'Images_Dataset.csv'
audio_csv_path = 'Processed_Ravdess/RAVDESS_Dataset.csv'

img_size = (128, 128)
n_mfcc = 13
max_length = 200
wav2vec_dim = 768
num_emotions = 7
num_threat_levels = 2

print("Loading preprocessed data")
images, mfccs, wav2vec_features, labels, threat_levels, le_emotion, le_threat = load_preprocessed_data(
    image_csv_path=image_csv_path,
    audio_csv_path=audio_csv_path,
    img_size=img_size,
    max_length=max_length
)

print("Splitting data into train and test sets")
(
    images_train, images_test,
    mfccs_train, mfccs_test,
    wav2vec_train, wav2vec_test,
    labels_train, labels_test,
    threat_train, threat_test
) = train_test_split(
    images, mfccs, wav2vec_features, labels, threat_levels,
    test_size=0.2, random_state=42
)

print("Building multimodal model")
model = build_multimodal_model(
    img_size=img_size,
    n_mfcc=n_mfcc,
    max_length=max_length,
    wav2vec_dim=wav2vec_dim,
    num_emotions=num_emotions,
    num_threat_levels=num_threat_levels
)
model.summary()

print("Training model")
history = train_multimodal_model(
    model=model,
    images=images_train,
    mfccs=mfccs_train,
    wav2vec_features=wav2vec_train,
    labels=labels_train,
    threat_levels=threat_train,
    batch_size=32,
    epochs= 50 ,
    validation_split=0.2
)

print("Evaluating model")
metrics = evaluate_multimodal_model(
    model=model,
    images=images_test,
    mfccs=mfccs_test,
    wav2vec_features=wav2vec_test,
    labels=labels_test,
    threat_levels=threat_test
)

print("Evaluation Metrics:")
print(f"Emotion Loss: {metrics['emotion_loss']:.4f}")
print(f"Threat Loss: {metrics['threat_loss']:.4f}")
print(f"Emotion Accuracy: {metrics['emotion_accuracy']:.4f}")
print(f"Threat Accuracy: {metrics['threat_accuracy']:.4f}")

#!pip install scikit-image==0.19.3

from lime import lime_image
from lime.wrappers.scikit_image import SegmentationAlgorithm
from skimage.segmentation import mark_boundaries

def grad_cam(model, img, mfcc, wav2vec, layer_name='conv2d_last', output_name='emotion_output', class_idx=None):

    img = np.expand_dims(img, axis=0)
    mfcc = np.expand_dims(mfcc, axis=0)
    wav2vec = np.expand_dims(wav2vec, axis=0)

    grad_model = Model(
        inputs=model.inputs,
        outputs=[model.get_layer(layer_name).output, model.get_layer(output_name).output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model([img, mfcc, wav2vec])
        if class_idx is None:
            class_idx = tf.argmax(predictions[0])
        loss = predictions[:, class_idx]

    grads = tape.gradient(loss, conv_outputs)[0]
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]

    heatmap = tf.reduce_mean(conv_outputs * pooled_grads, axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap = heatmap / (np.max(heatmap) + 1e-10)

    return heatmap

def visualize_grad_cam(img, heatmap, title):

    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    img_uint8 = np.uint8(255 * img[..., 0])
    superimposed_img = heatmap * 0.4 + cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR) * 0.6
    plt.figure(figsize=(8, 8))
    plt.imshow(superimposed_img.astype('uint8'))
    plt.title(title)
    plt.axis('off')
    plt.show()

def feature_importance_perturbation(model, audio_features, img, mfcc, wav2vec, input_name='mfcc_input', output_name='emotion_output', num_perturbations=100):

    img = np.expand_dims(img, axis=0)
    mfcc = np.expand_dims(mfcc, axis=0)
    wav2vec = np.expand_dims(wav2vec, axis=0)

    baseline_prediction = model.predict([img, mfcc, wav2vec], verbose=0)[model.output_names.index(output_name)]

    num_perturbations = min(num_perturbations, audio_features.shape[-1] if input_name == 'wav2vec_input' else audio_features.shape[0])  # Adjust for MFCC shape

    importance_scores = []
    for i in range(num_perturbations):
        perturbed_features = audio_features.copy()

        if input_name == 'mfcc_input':
            perturbed_features[i, :] = 0
        else:
            perturbed_features[i] = 0

        if input_name == 'mfcc_input':
            perturbed_features = perturbed_features[np.newaxis, ...]
        else:
            perturbed_features = perturbed_features[np.newaxis, ...]

        perturbed_prediction = model.predict(
            [img, mfcc if input_name == 'wav2vec_input' else perturbed_features, wav2vec if input_name == 'mfcc_input' else perturbed_features],  # Adjust input order
            verbose=0
        )[model.output_names.index(output_name)]

        importance = np.mean(np.abs(baseline_prediction - perturbed_prediction))
        importance_scores.append(importance)

    return np.array(importance_scores)

def visualize_feature_importance(importance_scores, title, feature_dim, feature_type='MFCC Coefficient'):

    plt.figure(figsize=(12, 4))
    plt.bar(np.arange(feature_dim), importance_scores, color='r')
    plt.title(title)
    plt.xlabel(feature_type)
    plt.ylabel('Importance')
    plt.grid(True)
    plt.show()

def Apply_LIME(Model, Image, Num_Samples=1000):

    print("Generating LIME explanation.")
    print("\n")
    Explainer = lime_image.LimeImageExplainer()
    def Predict_Fn(Images):
        Images = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in Images])
        Images = Images[..., np.newaxis]
        Dummy_Audio = np.zeros((len(Images), 13, 200))
        Dummy_Wav = np.zeros((len(Images), 300, 768))
        predictions = Model.predict([Images, Dummy_Audio, Dummy_Wav])[0]
        return predictions

    Image_RGB = cv2.cvtColor(Image[0].astype(np.uint8), cv2.COLOR_GRAY2RGB)
    Explanation = Explainer.explain_instance(
        Image_RGB,
        Predict_Fn,
        top_labels=5,
        num_samples=Num_Samples
    )
    Temp, Mask = Explanation.get_image_and_mask(
        Explanation.top_labels[0],
        positive_only=True,
        num_features=5,
        hide_rest=True
    )
    Marked = mark_boundaries(Temp / Temp.max(), Mask)
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(Image[0])
    plt.title('Original Image')
    plt.subplot(1, 2, 2)
    plt.imshow(Marked)
    plt.title('LIME Explanation')
    plt.tight_layout()
    plt.show()
    print("LIME explanation completed.")
    print("\n")
    return Marked

def Visualize_Wav2Vec_Features(Wav2Vec_Features, Label):

    print("Generating Wav2Vec feature importance.")
    print("\n")
    Mean_Features = np.mean(Wav2Vec_Features, axis=1)
    plt.figure(figsize=(12, 4))
    plt.plot(Mean_Features)
    plt.title(f'Wav2Vec Feature Importance for {Label}')
    plt.xlabel('Feature Dimension')
    plt.ylabel('Mean Activation')
    plt.show()
    print("Wav2Vec feature visualization completed.")
    print("\n")

sample_idx = 65
test_img = images_test[sample_idx]
test_mfcc = mfccs_test[sample_idx]
test_wav2vec = wav2vec_test[sample_idx]

print("Generating Grad-CAM explanation")
heatmap = grad_cam(
    model, test_img, test_mfcc, test_wav2vec,
    layer_name='conv2d_1', output_name='emotion_output'
)
visualize_grad_cam(test_img, heatmap, "Grad-CAM: Emotion Prediction")

print("Applying LIME.")
Marked = Apply_LIME(model, [test_img], Num_Samples=500)
print("LIME application completed.\n")

print("Generating MFCC feature importance")
mfcc_importance = feature_importance_perturbation(
model, test_mfcc, test_img, test_mfcc, test_wav2vec,
input_name='mfcc_input', output_name='emotion_output', num_perturbations=n_mfcc
)
visualize_feature_importance(mfcc_importance, "MFCC Feature Importance", n_mfcc, "MFCC Coefficient")

print("Generating Wav2Vec feature importance")
wav2vec_importance = feature_importance_perturbation(
model, test_wav2vec, test_img, test_mfcc, test_wav2vec,
input_name='wav2vec_input', output_name='emotion_output', num_perturbations=50
)
visualize_feature_importance(wav2vec_importance, "Wav2Vec Feature Importance", 50, "Wav2Vec Dimension")

Trained_Model = build_multimodal_model()
df_Face = pd.read_csv('Images_Dataset.csv')
df_Audio = pd.read_csv('Processed_Ravdess/RAVDESS_Dataset.csv')
Label_Encoder = LabelEncoder().fit(df_Audio['Labels'].unique())

Sample_Idx = 0
Sample_Image = np.expand_dims(cv2.resize(cv2.cvtColor(cv2.imread(df_Face['Image_Paths'].iloc[Sample_Idx]), cv2.COLOR_BGR2RGB), (128, 128)) / 255.0, axis=0)
Sample_Audio = np.expand_dims(np.load(df_Audio['Feature_Paths'].iloc[Sample_Idx]), axis=0)
Sample_Wav2Vec = np.expand_dims(np.load(df_Audio['Wav2Vec_Paths'].iloc[Sample_Idx]), axis=0)
Sample_Label = df_Audio['Labels'].iloc[Sample_Idx]
Sample_Class_Idx = Label_Encoder.transform([Sample_Label])[0]

print("Visualizing Wav2Vec features.")
Visualize_Wav2Vec_Features(Sample_Wav2Vec[0], Sample_Label)