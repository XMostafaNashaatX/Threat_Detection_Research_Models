# -*- coding: utf-8 -*-
"""AhmedAbdelsamad_202201913_M1_XAI_(1)_(1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qwl-vOv5HAfT2t3NBarg4RErBN2tvjlg

#A Victim Tracking System by Recognizing Signs of Violence Using Long-Short Term Memory.
"""

!pip install tf-keras-vis lime mtcnn transformers

import os
from pathlib import Path
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, ConvLSTM2D, Attention, TimeDistributed, Concatenate, LSTM
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.inspection import permutation_importance
from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus
from tf_keras_vis.utils.model_modifiers import ReplaceToLinear
from tf_keras_vis.utils.scores import CategoricalScore
import shap
from lime import lime_image
from skimage.segmentation import mark_boundaries
from skimage.feature import hog
from skimage import exposure
import kagglehub
import gc
from mtcnn import MTCNN
#from feat.detector import Detector
#from gaze_tracking import GazeTracking
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torch
from tf_keras_vis.utils.scores import BinaryScore

!pip install librosa

import librosa
import librosa.display

def Preprocess_Real_Life_Violence_Dataset(DATA_DIR, Output_DIR, Img_Size=(416, 416), Sample_Frac=0.1, Max_Frames=30, Fps_Target=30, Grayscale=True):

    Frame_DIR = os.path.join(Output_DIR, 'Frames')
    Face_DIR = os.path.join(Output_DIR, 'Faces')
    os.makedirs(Frame_DIR, exist_ok=True)
    os.makedirs(Face_DIR, exist_ok=True)

    print("Preprocessing RLVD Video Dataset:")

    detector = MTCNN()

    Data = {
        'Video_Paths': [],
        'Frame_Paths': [],
        'Face_Paths': [],
        'Labels': [],
        'Threat_Level': [],
        'Video_Name': []
    }

    Threat_Levels = {
        'NonViolence': 'Low',
        'Violence': 'High'
    }

    Emotion_Mapping = {
        'Violence': ['angry', 'fearful'],
        'NonViolence': ['neutral', 'calm']
    }

    Video_Files = []
    for root, _, files in os.walk(DATA_DIR):
        Video_Files.extend(os.path.join(root, f) for f in files if f.endswith(('.mp4', '.avi', '.mov', '.mp')))

    print(f"Found {len(Video_Files)} video files in {DATA_DIR}")
    print("\n")

    if not Video_Files:
        print("No video files found in the dataset.")
        return pd.DataFrame(Data)

    Num_Samples = max(1, int(len(Video_Files) * Sample_Frac))
    Sampled_Videos = np.random.choice(len(Video_Files), size=Num_Samples, replace=False)

    for Idx in tqdm(Sampled_Videos, desc="Processing Videos"):
        Video_Path = Video_Files[Idx]
        Video_Name = os.path.splitext(os.path.basename(Video_Path))[0]

        if "Real Life Violence Dataset/Violence" in Video_Path:
            Base_Label = "Violence"
        elif "Real Life Violence Dataset/NonViolence" in Video_Path:
            Base_Label = "NonViolence"
        else:
            Base_Label = "Violence" if (Video_Name.startswith("V_") and not Video_Name.startswith("NV_")) else "NonViolence"

        Label = np.random.choice(Emotion_Mapping[Base_Label])

        print(f"Processing video: {Video_Path}, Base Label: {Base_Label}, Emotion Label: {Label}")

        Video_Frame_Dir = os.path.join(Frame_DIR, Video_Name)
        Video_Face_Dir = os.path.join(Face_DIR, Video_Name)
        os.makedirs(Video_Frame_Dir, exist_ok=True)
        os.makedirs(Video_Face_Dir, exist_ok=True)

        Video = cv2.VideoCapture(Video_Path)
        if not Video.isOpened():
            print(f"Failed to open video: {Video_Path}")
            continue

        Frame_Count = int(Video.get(cv2.CAP_PROP_FRAME_COUNT))
        FPS = Video.get(cv2.CAP_PROP_FPS)
        if FPS <= 0:
            FPS = 30
        print(f"Frame count: {Frame_Count}, FPS: {FPS}")

        if Frame_Count <= 0:
            print(f"Invalid frame count for {Video_Path}: {Frame_Count}")
            Video.release()
            continue

        Sample_Interval = max(1, int(round(FPS / Fps_Target)))
        print(f"Sample interval (based on FPS {Fps_Target}): {Sample_Interval}")

        Frame_Idx = 0
        Saved_Frames = 0
        while Video.isOpened() and Saved_Frames < Max_Frames:
            Ret, Frame = Video.read()
            if not Ret:
                print(f"Failed to read frame at index {Frame_Idx} in {Video_Path}")
                break
            if Frame is None or Frame.size == 0:
                print(f"Invalid frame at index {Frame_Idx} in {Video_Path}")
                Frame_Idx += 1
                continue

            if Frame_Idx % Sample_Interval == 0:

                Frame_Resized = cv2.resize(Frame, Img_Size)

                Faces = detector.detect_faces(Frame_Resized)
                Face_Img = Frame_Resized
                if Faces:
                    x, y, w, h = Faces[0]['box']
                    Face_Img = Frame_Resized[max(0, y):y+h, max(0, x):x+w]
                    try:
                        Face_Img = cv2.resize(Face_Img, Img_Size)
                    except Exception as e:
                        print(f"Failed to resize face for frame {Frame_Idx} in {Video_Path}: {e}")
                        Frame_Idx += 1
                        continue
                else:
                    print(f"No face detected in frame {Frame_Idx} for {Video_Path}, using full frame")

                if Grayscale:
                    Frame_Processed = cv2.cvtColor(Frame_Resized, cv2.COLOR_BGR2GRAY)
                    Frame_Processed = Frame_Processed[..., np.newaxis]
                    Face_Processed = cv2.cvtColor(Face_Img, cv2.COLOR_BGR2RGB)
                    Face_Processed = cv2.cvtColor(Face_Processed, cv2.COLOR_RGB2GRAY)
                    Face_Processed = Face_Processed[..., np.newaxis]
                else:
                    Frame_Processed = Frame_Resized
                    Face_Processed = Face_Img

                Frame_Processed = Frame_Processed / 255.0
                Face_Processed = Face_Processed / 255.0

                Frame_Path = os.path.join(Video_Frame_Dir, f"Frame_{Saved_Frames}.jpg")
                Face_Path = os.path.join(Video_Face_Dir, f"Face_{Saved_Frames}.jpg")
                Frame_To_Save = (Frame_Processed * 255).astype(np.uint8)
                Face_To_Save = (Face_Processed * 255).astype(np.uint8)
                if Grayscale:
                    Frame_To_Save = Frame_To_Save.squeeze()
                    Face_To_Save = Face_To_Save.squeeze()
                print(f"Saving frame: {Frame_Path}")
                print(f"Saving face: {Face_Path}")
                if not cv2.imwrite(Frame_Path, Frame_To_Save):
                    print(f"Failed to write frame: {Frame_Path}")
                    Frame_Idx += 1
                    continue
                if not cv2.imwrite(Face_Path, Face_To_Save):
                    print(f"Failed to write face: {Face_Path}")
                    Frame_Idx += 1
                    continue

                Data['Video_Paths'].append(Video_Path)
                Data['Frame_Paths'].append(Frame_Path)
                Data['Face_Paths'].append(Face_Path)
                Data['Threat_Level'].append(Threat_Levels[Base_Label])
                Data['Labels'].append(Label)
                Data['Video_Name'].append(Video_Name)
                Saved_Frames += 1

            Frame_Idx += 1

        Video.release()
        print(f"Processed {Saved_Frames} frames for {Video_Path}")
        print("\n")

    df = pd.DataFrame(Data)
    print(f"Processed {len(df)} frames from {len(set(Data['Video_Paths']))} videos.")
    print(f"Label distribution:\n{df['Labels'].value_counts()}")
    print(f"Threat level distribution:\n{df['Threat_Level'].value_counts()}")
    print("\n")

    Output_Data_path = os.path.join(Output_DIR, 'Videos_Dataset.csv')
    df.to_csv(Output_Data_path, index=False)
    print(f"Metadata saved to {Output_Data_path}")
    print("\n")

    return df

def EDA_RLVD(Dataset_Path , Num_Visualizations=5):

    df = pd.read_csv(Dataset_Path)

    print("Label Distribution:")
    #print(df['Labels'].value_counts())
    #print("\n")
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='Labels' , hue='Labels')
    plt.title('Label Distribution')
    plt.xlabel('Labels')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nThreat Level Distribution:")
    #print(df['Threat_Level'].value_counts())
    #print("\n")
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='Threat_Level' , hue='Threat_Level')
    plt.title('Threat Level Distribution')
    plt.xlabel('Threat Level')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nVideo Length Distribution:")
    df['Video_Length'] = df['Video_Paths'].apply(lambda x:
    cv2.VideoCapture(x).get(cv2.CAP_PROP_FRAME_COUNT)
    / max(1, cv2.VideoCapture(x).get(cv2.CAP_PROP_FPS)))
    plt.figure(figsize=(8, 6))
    sns.histplot(data=df, x='Video_Length', bins=30, kde=True)
    plt.title('Video Length Distribution')
    plt.xlabel('Video Length (seconds)')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nFrames per Video Statistics :")
    Frame_Counts = df.groupby('Video_Name').size()
    print(Frame_Counts.describe())
    print("\n")
    df['Frame_Dir'] = df['Frame_Paths'].apply(lambda x: os.path.dirname(x))
    Frames_Per_Video = df.groupby('Frame_Dir')['Frame_Dir'].transform('count')
    print(f"Minimum frames per video: {Frames_Per_Video.min()}")
    print(f"Maximum frames per video: {Frames_Per_Video.max()}")
    print(f"Mean frames per video: {Frames_Per_Video.mean()}")
    print(f"Median frames per video: {Frames_Per_Video.median()}")
    print("\n")

    plt.figure(figsize=(8, 6))
    sns.histplot(Frames_Per_Video , bins=30, kde=True)
    plt.title('Frames per Video Distribution')
    plt.xlabel('Frames per Video')
    plt.ylabel('Count')
    plt.show()
    print("\n")

    print("\nVisualizing Sample Frames :")

    Sample_Frames = df.sample(n=Num_Visualizations, random_state=42)

    for Idx , Frame_Path in enumerate(Sample_Frames['Frame_Paths']):
        Frame = cv2.imread(Frame_Path)
        Frame = cv2.cvtColor(Frame, cv2.COLOR_BGR2RGB)

        if Frame is None:
            print(f"Failed to read frame: {Frame_Path}")
            continue

        plt.figure(figsize=(8, 6))
        plt.imshow(Frame)
        plt.title(f'Sample Frame {Idx + 1}')
        plt.axis('off')
        plt.show()
        print("\n")

    print("\n")

!pip install fer -q

from fer import FER

def Extract_FER_Emotions(df, Output_DIR, img_size=(224, 224)):

    print("Extracting Facial Emotion Recognition (FER) Probabilities:")

    Emotion_DIR = os.path.join(Output_DIR, 'Features', 'Emotions')
    os.makedirs(Emotion_DIR, exist_ok=True)

    detector = FER(mtcnn=True)
    Emotion_Paths = []

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing Emotion Extraction"):
        video_name = row['Video_Name']
        face_paths = eval(row['Face_Paths']) if isinstance(row['Face_Paths'], str) else row['Face_Paths']

        print(f"Processing video: {video_name}")

        Emotion_Probs = []
        for frame_path in face_paths:
            frame = cv2.imread(frame_path)
            if frame is None:
                print(f"Failed to read frame: {frame_path}")
                Emotion_Probs.append(np.zeros(7))
                continue

            frame = cv2.resize(frame, img_size)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            try:
                result = detector.detect_emotions(frame_rgb)
                if result and len(result) > 0:
                    emotions = result[0]['emotions']
                    probs = np.array([emotions[key] for key in ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']])
                else:
                    probs = np.zeros(7)
            except Exception as e:
                print(f"Failed to extract emotions for {frame_path}: {e}")
                probs = np.zeros(7)

            Emotion_Probs.append(probs)

        Emotion_Path = os.path.join(Emotion_DIR, f"{video_name}_emotions.npy")
        np.save(Emotion_Path, np.array(Emotion_Probs))
        Emotion_Paths.append(Emotion_Path)
        print(f"Saved emotion probabilities: {Emotion_Path}")

    df['Emotion_Paths'] = Emotion_Paths
    print(f"Extracted emotion probabilities for {len(df)} videos.")
    print("\n")

    return df

def Feature_Engineering_RLVD(dataset_path, Output_DIR):

    print("Starting RLVD Feature Engineering:")

    df = pd.read_csv(dataset_path)
    print(f"Loaded dataset with {len(df)} samples.")
    print("\n")

    df = df.groupby('Video_Name').agg({
        'Video_Paths': 'first',
        'Frame_Paths': list,
        'Face_Paths': list,
        'Labels': 'first',
        'Threat_Level': 'first'
    }).reset_index()

    print(f"Aggregated dataset to {len(df)} unique videos.")
    print("\n")

    df = Extract_FER_Emotions(df, Output_DIR)

    Output_Path = os.path.join(Output_DIR, 'Videos_Dataset_Features.csv')
    df.to_csv(Output_Path, index=False)
    print(f"Saved feature-engineered dataset to {Output_Path}")
    print("\n")

    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='Threat_Level')
    plt.title("Distribution of Threat Levels")
    plt.xlabel("Threat Level")
    plt.ylabel("Count")
    plt.show()
    print("\n")

    return df

import os
import numpy as np
import pandas as pd
import cv2
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus
from tf_keras_vis.utils.model_modifiers import ReplaceToLinear
from tf_keras_vis.utils.scores import BinaryScore
import shap
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
import gc
import ast

def Count_Valid_Sequences(video_groups, sequence_length=10):

    print("Counting Valid Sequences:")
    valid_sequences = 0
    for name, group in video_groups:
        frame_paths = sorted(group['Frame_Paths'].values)
        if len(frame_paths) >= sequence_length:
            valid_sequences += (len(frame_paths) - sequence_length + 1) // sequence_length
        print(f"Video {name}: {len(frame_paths)} frames")

    print(f"Found {valid_sequences} valid sequences.")
    print("\n")
    return valid_sequences

def Video_Sequence_Generator(video_groups, sequence_length=10, img_size=(224, 224), batch_size=4, grayscale=True):

    print("Preparing Sequence Generator:")
    sequences = []
    for name, group in video_groups:
        frame_paths = sorted(group['Frame_Paths'].values)
        label = 1 if group['Threat_Level'].iloc[0] == 'High' else 0
        if len(frame_paths) >= sequence_length:
            for i in range(0, len(frame_paths) - sequence_length + 1, sequence_length):
                sequences.append((frame_paths[i:i + sequence_length], label))

    print(f"Generated {len(sequences)} sequences for processing.")
    print("\n")

    indices = np.arange(len(sequences))
    np.random.shuffle(indices)

    for idx in range(0, len(sequences), batch_size):
        batch_indices = indices[idx:idx + batch_size]
        batch_sequences = [sequences[i] for i in batch_indices]
        X, y = [], []

        print(f"Processing Batch {idx//batch_size + 1}:")
        for frame_paths, label in tqdm(batch_sequences, desc="Loading Frames", total=len(batch_sequences)):
            seq_frames = []
            try:
                for path in frame_paths:
                    if not os.path.exists(path):
                        print(f"Frame path does not exist: {path}")
                        break
                    frame = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
                    if frame is None:
                        print(f"Failed to read frame: {path}")
                        break
                    frame = cv2.resize(frame, img_size) / 255.0
                    if np.random.rand() > 0.5:
                        frame = cv2.flip(frame, 1)
                    frame = frame[..., np.newaxis]
                    seq_frames.append(frame)

                if len(seq_frames) == sequence_length:
                    X.append(np.array(seq_frames))
                    y.append(label)
            except Exception as e:
                print(f"Error processing sequence: {e}")
                continue

        if X:
            print(f"Yielding batch with {len(X)} sequences.")
            yield np.array(X), np.array(y)
        else:
            print("No valid sequences in batch.")
        print("\n")

def Preprocess_Sequences(dataset_path, sequence_length=10, img_size=(224, 224), grayscale=True):

    print("Preprocessing Sequences:")
    df = pd.read_csv(dataset_path)
    video_groups = df.groupby('Video_Name')

    X, y = [], []
    sequence_count = 0

    for name, group in tqdm(video_groups, desc="Processing Videos"):
        frame_paths = sorted(group['Frame_Paths'].values)
        if len(frame_paths) >= sequence_length:
            for i in range(0, len(frame_paths) - sequence_length + 1, sequence_length):
                seq_frames = []
                try:
                    for path in frame_paths[i:i + sequence_length]:
                        if not os.path.exists(path):
                            print(f"Frame path does not exist: {path}")
                            continue
                        frame = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
                        if frame is None:
                            print(f"Failed to read frame: {path}")
                            continue
                        frame = cv2.resize(frame, img_size) / 255.0
                        frame = frame[..., np.newaxis]
                        seq_frames.append(frame)
                    if len(seq_frames) == sequence_length:
                        X.append(np.array(seq_frames))
                        y.append(1 if group['Threat_Level'].iloc[0] == 'High' else 0)
                        sequence_count += 1
                except Exception as e:
                    print(f"Error processing sequence for video {name}: {e}")
                    continue

    print(f"Processed {sequence_count} valid sequences.")
    print("\n")
    return np.array(X), np.array(y)

def Build_Model(input_shape=(10, 224, 224, 1), num_classes=1):

    print("Building CNN-LSTM Model:")
    inputs = Input(shape=input_shape, name='video_input')
    x = TimeDistributed(
        tf.keras.Sequential([
            Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2d_1'),
            BatchNormalization(),
            MaxPooling2D((2, 2)),
            Dropout(0.3),
            Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2d_2'),
            BatchNormalization(),
            MaxPooling2D((2, 2)),
            Dropout(0.3),
            Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d_3'),
            BatchNormalization(),
            MaxPooling2D((2, 2)),
            Flatten()
        ], name='cnn')
    )(inputs)

    x = LSTM(32, return_sequences=False, name='lstm')(x)
    x = Dropout(0.5)(x)
    x = Dense(16, activation='relu', name='dense_1', kernel_regularizer=l2(0.01))(x)
    outputs = Dense(num_classes, activation='sigmoid', name='output')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    print("Model Summary:")
    model.summary()
    print("\n")
    return model

def Train_Model(model, dataset_path, sequence_length=10, img_size=(224, 224), batch_size=4, epochs=20):

    print("Preparing Data for Training:")
    try:
        df = pd.read_csv(dataset_path)
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return model

    if df.empty:
        print("Error: Dataset is empty.")
        return model

    video_groups = list(df.groupby('Video_Name'))
    valid_sequences = Count_Valid_Sequences(video_groups, sequence_length)

    if valid_sequences < batch_size:
        print(f"Invalid sequences ({valid_sequences}) for batch_size ({batch_size}).")
        return model

    print("Splitting Data:")
    train_idx, val_idx = train_test_split(range(len(video_groups)), test_size=0.2, random_state=42)
    train_groups = [video_groups[i] for i in train_idx]
    val_groups = [video_groups[i] for i in val_idx]

    train_sequences = Count_Valid_Sequences(train_groups, sequence_length)
    val_sequences = Count_Valid_Sequences(val_groups, sequence_length)

    print(f"Training sequences: {train_sequences}")
    print(f"Validation sequences: {val_sequences}")
    print("\n")

    print("Starting Model Training:")
    train_generator = Video_Sequence_Generator(train_groups, sequence_length, img_size, batch_size, grayscale=True)
    val_generator = Video_Sequence_Generator(val_groups, sequence_length, img_size, batch_size, grayscale=True)

    steps_per_epoch = max(1, train_sequences // batch_size)
    validation_steps = max(1, val_sequences // batch_size)

    model.fit(
        train_generator,
        steps_per_epoch=steps_per_epoch,
        validation_data=val_generator,
        validation_steps=validation_steps,
        epochs=epochs,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
            tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)
        ]
    )

    print("Training Complete.")
    gc.collect()
    print("\n")
    return model

def Test_Model(model, dataset_path, sequence_length=10, img_size=(224, 224), batch_size=4):

    print("Testing Model:")
    try:
        df = pd.read_csv(dataset_path)
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return {}

    video_groups = list(df.groupby('Video_Name'))
    valid_sequences = Count_Valid_Sequences(video_groups, sequence_length)

    if valid_sequences < batch_size:
        print(f"Invalid sequences ({valid_sequences}) for batch_size ({batch_size}).")
        return {}

    print(f"Found {valid_sequences} sequences for testing.")
    print("\n")

    print("Generating Predictions:")
    test_generator = Video_Sequence_Generator(video_groups, sequence_length, img_size, batch_size, grayscale=True)
    y_true, y_pred = [], []

    steps = max(1, valid_sequences // batch_size)
    for batch_idx, (X, y) in enumerate(tqdm(test_generator, desc="Processing Test Batches", total=steps)):
        if X.size == 0:
            print("Empty batch encountered, skipping.")
            continue
        y_true.extend(y)
        y_pred.extend((model.predict(X, verbose=0) > 0.5).astype(int).flatten())
        if batch_idx + 1 >= steps:
            break

    if not y_true:
        print("Invalid test sequences found.")
        return {}

    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0)
    }

    print("Test Metrics:")
    for metric, value in metrics.items():
        print(f"{metric.capitalize()}: {value:.4f}")
    print("\n")

    gc.collect()
    return metrics

DATA_DIR = kagglehub.dataset_download("mohamedmustafa/real-life-violence-situations-dataset")
OUTPUT_DIR = 'Processed_Real_Life_Violence'
Videos_RLVD_Dataset = Preprocess_Real_Life_Violence_Dataset(DATA_DIR, OUTPUT_DIR, Img_Size=(224, 224), Sample_Frac=0.02, Max_Frames=30, Fps_Target=30, Grayscale=True)
Dataset_Path = os.path.join(OUTPUT_DIR, 'Videos_Dataset.csv')

EDA_RLVD(Dataset_Path)

OUTPUT_DIR = 'Feature_Processed_RLVD'
Features_RLVD = Feature_Engineering_RLVD(Dataset_Path, OUTPUT_DIR)

dataset_path = '/content/Processed_Real_Life_Violence/Videos_Dataset.csv'

model = Build_Model(input_shape=(10, 224, 224, 1), num_classes=1)
model = Train_Model(model, dataset_path, sequence_length=10, img_size=(224, 224), batch_size=4, epochs=20)

metrics = Test_Model(model, dataset_path, sequence_length=10, img_size=(224, 224), batch_size=4)
print("Test Metrics:", metrics)